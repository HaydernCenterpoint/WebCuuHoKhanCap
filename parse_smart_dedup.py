import re
import json
from difflib import SequenceMatcher

def normalize_text(text):
    """Chuáº©n hÃ³a text Ä‘á»ƒ so sÃ¡nh"""
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    return text.strip()

def normalize_phone(phone):
    """Chuáº©n hÃ³a sá»‘ Ä‘iá»‡n thoáº¡i"""
    return re.sub(r'\D', '', phone)

def similarity(a, b):
    """TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 2 chuá»—i"""
    return SequenceMatcher(None, normalize_text(a), normalize_text(b)).ratio()

def extract_address_core(content):
    """TrÃ­ch xuáº¥t pháº§n Ä‘á»‹a chá»‰ cá»‘t lÃµi tá»« content"""
    # Loáº¡i bá» pháº§n mÃ´ táº£ tÃ¬nh tráº¡ng
    address = re.split(r'[â€“\-â€”:]\s*', content)[0]
    # Loáº¡i bá» sá»‘ lÆ°á»£ng ngÆ°á»i
    address = re.sub(r'\d+\s*(ngÆ°á»i|em|bÃ©|chÃ¡u|Ä‘á»©a|con)', '', address)
    # Láº¥y pháº§n Ä‘áº§u (thÆ°á»ng lÃ  Ä‘á»‹a chá»‰)
    words = address.split()
    if len(words) > 10:
        address = ' '.join(words[:10])
    return normalize_text(address)

def is_duplicate(case1, case2, phone_threshold=0.5, address_threshold=0.7):
    """Kiá»ƒm tra 2 case cÃ³ trÃ¹ng láº·p khÃ´ng"""
    # Check phone numbers
    phones1 = set([normalize_phone(p) for p in case1['phones']])
    phones2 = set([normalize_phone(p) for p in case2['phones']])
    
    if phones1 and phones2:
        common_phones = phones1.intersection(phones2)
        if len(common_phones) > 0:
            return True
    
    # Check address similarity
    addr1 = extract_address_core(case1['content'])
    addr2 = extract_address_core(case2['content'])
    
    if addr1 and addr2 and len(addr1) > 5 and len(addr2) > 5:
        sim = similarity(addr1, addr2)
        if sim >= address_threshold:
            return True
    
    return False

def merge_duplicates(cases):
    """Gá»™p cÃ¡c case trÃ¹ng láº·p, giá»¯ láº¡i case cÃ³ priority cao nháº¥t"""
    priority_order = {'CRITICAL': 0, 'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}
    
    unique_cases = []
    skip_indices = set()
    
    for i, case in enumerate(cases):
        if i in skip_indices:
            continue
        
        # Find all duplicates of this case
        duplicates = [case]
        for j in range(i + 1, len(cases)):
            if j in skip_indices:
                continue
            if is_duplicate(case, cases[j]):
                duplicates.append(cases[j])
                skip_indices.add(j)
        
        # Merge duplicates: keep the one with highest priority
        if len(duplicates) > 1:
            best = min(duplicates, key=lambda c: priority_order.get(c['priority'], 99))
            
            # Merge phone numbers
            all_phones = set()
            for dup in duplicates:
                all_phones.update(dup['phones'])
            best['phones'] = sorted(list(all_phones))[:5]  # Keep max 5 phones
            
            unique_cases.append(best)
        else:
            unique_cases.append(case)
    
    return unique_cases

def parse_priority_from_line(line):
    """TrÃ­ch xuáº¥t má»©c Ä‘á»™ Æ°u tiÃªn tá»« dÃ²ng"""
    line_lower = line.lower()
    if 'kháº©n cáº¥p' in line_lower:
        return 'CRITICAL'
    elif 'Æ°u tiÃªn cao' in line_lower:
        return 'HIGH'
    elif 'thÆ°á»ng' in line_lower:
        return 'MEDIUM'
    return None

def parse_rescue_data_smart(input_file):
    """Parse dá»¯ liá»‡u cá»©u há»™ vá»›i lá»c trÃ¹ng láº·p thÃ´ng minh"""
    
    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    raw_cases = []
    current_id = 1
    
    # Phone pattern
    phone_pattern = r'0[\d\s\.]{8,}'
    
    # Danh sÃ¡ch khu vá»±c Ä‘áº§y Ä‘á»§
    area_keywords = {
        'BÃ n Tháº¡ch': 'BÃ n Tháº¡ch',
        'Báº¯c Nha Trang': 'Báº¯c Nha Trang',
        'Bá»‡nh Viá»‡n ÄÆ°á»ng Sáº¯t': 'BV ÄÆ°á»ng Sáº¯t',
        'Bá»‡nh viá»‡n Ä‘Æ°á»ng sáº¯t': 'BV ÄÆ°á»ng Sáº¯t',
        'BÃ¬nh KhÃ¡nh': 'BÃ¬nh KhÃ¡nh',
        'Cáº§u BÃ¨': 'Cáº§u BÃ¨',
        'Cáº§u Dá»©a': 'Cáº§u Dá»©a',
        'Cáº§u Gá»—': 'Cáº§u Gá»—',
        'Cáº§u KÃ©': 'Cáº§u KÃ©',
        'CÃ¢y Dáº§u ÄÃ´i': 'CÃ¢y Dáº§u ÄÃ´i',
        'DiÃªn Äiá»n': 'DiÃªn Äiá»n',
        'DiÃªn HÃ²a': 'DiÃªn HÃ²a',
        'DiÃªn KhÃ¡nh': 'DiÃªn KhÃ¡nh',
        'DiÃªn PhÃº': 'DiÃªn PhÃº',
        'GÃ² CÃ¢y Sung': 'GÃ² CÃ¢y Sung',
        'LÆ°Æ¡ng Äá»‹nh Cá»§a': 'VÄ©nh Ngá»c',
        'PhÃº NÃ´ng': 'PhÃº NÃ´ng',
        'TÃ¢y Nha Trang': 'VÄ©nh Ngá»c',
        'VÄ©nh ChÃ¢u': 'VÄ©nh ChÃ¢u',
        'VÄ©nh Hiá»‡p': 'VÄ©nh Hiá»‡p',
        'VÄ©nh Ngá»c': 'VÄ©nh Ngá»c',
        'VÄ©nh PhÆ°Æ¡ng': 'VÄ©nh PhÆ°Æ¡ng',
        'VÄ©nh ThÃ¡i': 'VÄ©nh ThÃ¡i',
        'VÄ©nh Tháº¡nh': 'VÄ©nh Tháº¡nh',
        'VÄ©nh Trung': 'VÄ©nh Trung',
        'VÃµ Cáº¡nh': 'VÃµ Cáº¡nh',
        'VÃµ DÃµng': 'VÃµ DÃµng',
        'XuÃ¢n SÆ¡n': 'XuÃ¢n SÆ¡n',
        '23/10': 'ÄÆ°á»ng 23/10',
        'Ä‘Æ°á»ng 23': 'ÄÆ°á»ng 23/10',
    }
    
    for i, line in enumerate(lines):
        line = line.strip()
        
        # Skip header vÃ  dÃ²ng trá»‘ng
        if not line or 'Má»©c Ä‘á»™ Æ°u tiÃªn' in line or 'CHá»– NÃ€O' in line:
            continue
        
        # Parse priority
        priority = parse_priority_from_line(line)
        if not priority:
            priority = 'MEDIUM'
        
        # Extract phones
        phones = []
        phone_matches = re.findall(phone_pattern, line)
        
        for phone in phone_matches:
            clean_phone = re.sub(r'[^\d]', '', phone)
            if 9 <= len(clean_phone) <= 11:
                if len(clean_phone) == 10:
                    formatted = f"{clean_phone[:4]} {clean_phone[4:7]} {clean_phone[7:]}"
                elif len(clean_phone) == 9:
                    formatted = f"{clean_phone[:3]} {clean_phone[3:6]} {clean_phone[6:]}"
                else:
                    formatted = clean_phone
                phones.append(formatted)
        
        # Remove phones and priority keywords from content
        content = line
        for phone_match in phone_matches:
            content = content.replace(phone_match, '')
        content = re.sub(r'(Kháº©n cáº¥p|Æ¯u tiÃªn cao|ThÆ°á»ng)', '', content, flags=re.IGNORECASE)
        
        # Clean content
        content = re.sub(r'\s+', ' ', content).strip()
        content = re.sub(r'^[^\w]+', '', content)
        
        if not content or len(content) < 10:
            continue
        
        # Determine area
        area = 'KhÃ¡c'
        for keyword, area_name in area_keywords.items():
            if keyword.lower() in content.lower():
                area = area_name
                break
        
        # Create case
        case = {
            "id": current_id,
            "content": content,
            "phones": phones,
            "area": area,
            "priority": priority,
            "isRescued": False
        }
        
        raw_cases.append(case)
        current_id += 1
    
    print(f"ğŸ“ Parsed {len(raw_cases)} cases from PDF")
    
    # Deduplicate
    print("ğŸ” Removing duplicates...")
    unique_cases = merge_duplicates(raw_cases)
    
    duplicates_removed = len(raw_cases) - len(unique_cases)
    print(f"âœ… Removed {duplicates_removed} duplicate cases")
    
    # Re-assign IDs
    for i, case in enumerate(unique_cases, 1):
        case['id'] = i
    
    return unique_cases

# Parse dá»¯ liá»‡u
print("=" * 60)
print("ğŸš¨ RESCUE DATA PARSER - SMART DEDUPLICATION")
print("=" * 60)

data = parse_rescue_data_smart('pdf_content.txt')

# Statistics
priority_counts = {}
area_counts = {}

for case in data:
    priority = case['priority']
    priority_counts[priority] = priority_counts.get(priority, 0) + 1
    
    area = case['area']
    area_counts[area] = area_counts.get(area, 0) + 1

print(f"\nâœ… Total unique cases: {len(data)}")

print(f"\nğŸ“Š Priority Distribution:")
for priority in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
    count = priority_counts.get(priority, 0)
    if count > 0:
        emoji = 'ğŸš¨' if priority == 'CRITICAL' else 'âš ï¸' if priority == 'HIGH' else 'ğŸ“' if priority == 'MEDIUM' else 'â„¹ï¸'
        print(f"  {emoji} {priority}: {count} cases ({count/len(data)*100:.1f}%)")

print(f"\nğŸ—ºï¸  Area Distribution (All {len(area_counts)} areas):")
for area, count in sorted(area_counts.items(), key=lambda x: x[1], reverse=True):
    print(f"  {area}: {count} cases")

# Save to JSON
output_file = 'rescue-app/src/data.json'
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

print(f"\nâœ… Saved {len(data)} unique cases to {output_file}")

# Show top CRITICAL cases
print(f"\nğŸš¨ Top 5 CRITICAL cases:")
critical_cases = [c for c in data if c['priority'] == 'CRITICAL'][:5]
for i, case in enumerate(critical_cases, 1):
    phones_str = ', '.join(case['phones'][:2]) if case['phones'] else '(No phone)'
    print(f"\n{i}. [{case['area']}] {case['content'][:100]}...")
    print(f"   ğŸ“ {phones_str}")
