import re
import json
from difflib import SequenceMatcher

def normalize_text(text):
    """Chuáº©n hÃ³a text Ä‘á»ƒ so sÃ¡nh"""
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    return text.strip()

def normalize_phone(phone):
    """Chuáº©n hÃ³a sá»‘ Ä‘iá»‡n thoáº¡i"""
    return re.sub(r'\D', '', phone)

def similarity(a, b):
    """TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 2 chuá»—i"""
    return SequenceMatcher(None, normalize_text(a), normalize_text(b)).ratio()

def is_duplicate(case1, case2):
    """Kiá»ƒm tra trÃ¹ng láº·p"""
    # Check phones
    phones1 = set([normalize_phone(p) for p in case1['phones']])
    phones2 = set([normalize_phone(p) for p in case2['phones']])
    if phones1 and phones2 and phones1.intersection(phones2):
        return True
    
    # Check address similarity
    addr1 = case1['content'] # ÄÃ£ lÃ  strict address
    addr2 = case2['content']
    if len(addr1) > 5 and len(addr2) > 5:
        if similarity(addr1, addr2) > 0.85: # TÄƒng ngÆ°á»¡ng lÃªn cao hÆ¡n vÃ¬ address Ä‘Ã£ sáº¡ch
            return True
    return False

def extract_location_strict(text):
    """
    TrÃ­ch xuáº¥t Ä‘á»‹a Ä‘iá»ƒm theo quy táº¯c nghiÃªm ngáº·t (V2 - Enhanced).
    """
    # 0. Pre-clean: TÃ¡ch sá»‘ dÃ­nh liá»n vá»›i chá»¯ (VD: Thá»13Háº»m -> Thá» 13 Háº»m)
    # NhÆ°ng cáº©n tháº­n vá»›i Ä‘á»‹a chá»‰ sá»‘ (VD: 23/10, 14 Ä‘Æ°á»ng)
    # Logic: Náº¿u sá»‘ náº±m giá»¯a 2 kÃ½ tá»± thÆ°á»ng/hoa -> kháº£ nÄƒng cao lÃ  lá»—i dÃ­nh
    text = re.sub(r'([a-zA-Z])(\d+)([a-zA-Z])', r'\1 \2 \3', text)
    
    # 1. Loáº¡i bá» tiá»n tá»‘ Priority
    text = re.sub(r'^(Kháº©n cáº¥p|Æ¯u tiÃªn cao|ThÆ°á»ng)\s*[-:]?\s*', '', text, flags=re.IGNORECASE)
    
    # 2. Cáº¯t táº¡i dáº¥u phÃ¢n cÃ¡ch Máº NH
    for sep in ['.', '(', ':']:
        if sep in text: text = text.split(sep)[0]
        
    # 3. Xá»­ lÃ½ dáº¥u gáº¡ch ngang (-) vÃ  pháº©y (,)
    # TÃ¡ch thÃ nh cÃ¡c pháº§n, chá»‰ giá»¯ láº¡i pháº§n KHÃ”NG PHáº¢I lÃ  mÃ´ táº£
    parts = re.split(r'\s*[-â€“,]\s*', text)
    valid_parts = []
    
    desc_keywords = [
        'nhÃ ', 'cÃ³', 'nÆ°á»›c', 'bá»‹', 'káº¹t', 'ngáº­p', 'cáº§n', 'ngÆ°á»i', 'bÃ©', 'tráº»', 
        'Ã´ng', 'bÃ ', 'máº¹', 'bá»‘', 'gia Ä‘Ã¬nh', 'khu', 'dÃ£y', 'háº»m trá»', 'phÃ²ng trá»',
        'tÃ¬nh tráº¡ng', 'sÄ‘t', 'liÃªn há»‡', 'gáº¥p', 'kháº©n', 'máº¥t', 'khÃ´ng', 'tá»«', 'sau',
        'cáº¡nh', 'káº¿', 'Ä‘á»‘i diá»‡n', 'gáº§n', 'táº¡i', 'ngay', 'chá»—'
    ]
    
    for i, part in enumerate(parts):
        part = part.strip()
        if not part: continue
        
        # Náº¿u part báº¯t Ä‘áº§u báº±ng sá»‘ lÆ°á»£ng (VD: 13 ngÆ°á»i, 3 tráº» em) -> Dá»«ng
        if re.match(r'^\d+\s+(ngÆ°á»i|bÃ©|tráº»|em|con|bÃ |Ã´ng|gia Ä‘Ã¬nh)', part.lower()):
            break
            
        # Náº¿u part chá»©a tá»« khÃ³a mÃ´ táº£ á»Ÿ Ä‘áº§u -> Dá»«ng
        # Trá»« trÆ°á»ng há»£p lÃ  chá»‰ dáº«n Ä‘á»‹a lÃ½ há»£p lá»‡ (VD: Gáº§n cáº§u...)
        is_desc = False
        part_lower = part.lower()
        
        # Check keywords
        for k in desc_keywords:
            if part_lower.startswith(k):
                # Exception: "Gáº§n" + TÃªn riÃªng (Viáº¿t hoa) -> CÃ³ thá»ƒ lÃ  Ä‘á»‹a chá»‰
                if k == 'gáº§n' and i == 0: 
                    is_desc = False
                else:
                    is_desc = True
                break
        
        if is_desc: break
        valid_parts.append(part)
        
    text = ', '.join(valid_parts)
    
    # 4. Xá»­ lÃ½ láº·p tá»« (VD: 398/15 LÃª Äáº¡i CÆ°Æ¡ng398/15 LÄC)
    # TÃ¬m chuá»—i láº·p láº¡i dÃ i nháº¥t
    n = len(text)
    for length in range(10, n // 2 + 1):
        substr = text[:length]
        rest = text[length:]
        if substr in rest:
            # Náº¿u pháº§n láº·p láº¡i náº±m ngay sau -> Cáº¯t
            if rest.startswith(substr):
                text = substr
                break
            # Náº¿u láº·p láº¡i nhÆ°ng cÃ³ chÃºt rÃ¡c á»Ÿ giá»¯a -> Cáº¯t
            elif rest.strip().startswith(substr):
                text = substr
                break

    # 5. Cleanup cuá»‘i cÃ¹ng
    text = re.sub(r'\d{9,11}', '', text) # XÃ³a SÄT
    text = text.strip()
    text = text.strip('-,.')
    
    # 6. Validation: Äá»‹a chá»‰ quÃ¡ ngáº¯n hoáº·c chá»‰ toÃ n sá»‘ -> Bá»
    if len(text) < 4 or text.isdigit():
        return ""
        
    return text

def parse_strict():
    print("ğŸš€ Báº®T Äáº¦U TRÃCH XUáº¤T Äá»ŠA CHá»ˆ NGHIÃŠM NGáº¶T...")
    
    with open('pdf_content.txt', 'r', encoding='utf-8') as f:
        lines = f.readlines()
        
    raw_cases = []
    current_id = 1
    phone_pattern = r'0[\d\s\.]{8,}'
    
    # Area mapping (nhÆ° cÅ©)
    area_keywords = {
        'VÄ©nh Tháº¡nh': 'VÄ©nh Tháº¡nh', 'Chá»£ Ga': 'VÄ©nh Tháº¡nh', 'PhÃº BÃ¬nh': 'VÄ©nh Tháº¡nh',
        'VÄ©nh Ngá»c': 'VÄ©nh Ngá»c', 'LÆ°Æ¡ng Äá»‹nh Cá»§a': 'VÄ©nh Ngá»c', 'XuÃ¢n Láº¡c': 'VÄ©nh Ngá»c',
        'VÄ©nh Hiá»‡p': 'VÄ©nh Hiá»‡p', 'VÄ©nh ThÃ¡i': 'VÄ©nh ThÃ¡i', 'ThÃ¡i ThÃ´ng': 'VÄ©nh ThÃ¡i',
        'VÄ©nh Trung': 'VÄ©nh Trung', 'VÄ©nh PhÆ°Æ¡ng': 'VÄ©nh PhÆ°Æ¡ng', 'VÄ©nh Háº£i': 'VÄ©nh Háº£i',
        'Báº¯c Nha Trang': 'Báº¯c Nha Trang', 'ÄÆ°á»ng 23/10': 'ÄÆ°á»ng 23/10', 'Cáº§u BÃ¨': 'Cáº§u BÃ¨',
        'Cáº§u Dá»©a': 'Cáº§u Dá»©a', 'Cáº§u KÃ©': 'Cáº§u KÃ©', 'Cáº§u Gá»—': 'Cáº§u Gá»—', 'CÃ¢y Dáº§u ÄÃ´i': 'CÃ¢y Dáº§u ÄÃ´i',
        'GÃ² CÃ¢y Sung': 'GÃ² CÃ¢y Sung', 'PhÃº NÃ´ng': 'PhÃº NÃ´ng', 'Bá»‡nh Viá»‡n ÄÆ°á»ng Sáº¯t': 'BV ÄÆ°á»ng Sáº¯t',
        'BV ÄÆ°á»ng Sáº¯t': 'BV ÄÆ°á»ng Sáº¯t', 'Ngá»c Hiá»‡p': 'Ngá»c Hiá»‡p', 'PhÆ°á»›c Äá»“ng': 'PhÆ°á»›c Äá»“ng',
        'Äá»“ng Muá»‘i': 'PhÆ°á»›c Long', 'DiÃªn An': 'DiÃªn An', 'PhÃº Ã‚n Nam': 'DiÃªn An',
        'DiÃªn ToÃ n': 'DiÃªn ToÃ n', 'DiÃªn Thá»': 'DiÃªn Thá»', 'DiÃªn PhÆ°á»›c': 'DiÃªn PhÆ°á»›c',
        'DiÃªn Láº¡c': 'DiÃªn Láº¡c', 'DiÃªn SÆ¡n': 'DiÃªn SÆ¡n', 'DiÃªn LÃ¢m': 'DiÃªn LÃ¢m',
        'DiÃªn TÃ¢n': 'DiÃªn TÃ¢n', 'DiÃªn Äiá»n': 'DiÃªn Äiá»n', 'DiÃªn PhÃº': 'DiÃªn PhÃº',
        'DiÃªn HÃ²a': 'DiÃªn HÃ²a', 'BÃ¬nh KhÃ¡nh': 'DiÃªn HÃ²a', 'DiÃªn KhÃ¡nh': 'DiÃªn KhÃ¡nh',
        'Suá»‘i Hiá»‡p': 'Suá»‘i Hiá»‡p', 'BÃ n Tháº¡ch': 'BÃ n Tháº¡ch', 'VÃµ Cáº¡nh': 'VÃµ Cáº¡nh',
        'VÃµ DÃµng': 'VÃµ DÃµng', 'XuÃ¢n SÆ¡n': 'XuÃ¢n SÆ¡n',
    }
    sorted_areas = sorted(area_keywords.keys(), key=len, reverse=True)

    for line in lines:
        line = line.strip()
        if not line or 'Má»©c Ä‘á»™ Æ°u tiÃªn' in line or 'CHá»– NÃ€O' in line: continue
        
        # 1. Parse Priority
        priority = 'MEDIUM'
        line_lower = line.lower()
        if any(k in line_lower for k in ['kháº©n cáº¥p', 'nguy ká»‹ch', 'sáº¯p Ä‘áº»', 'vá»¡ á»‘i', 'tai biáº¿n']): priority = 'CRITICAL'
        elif any(k in line_lower for k in ['Æ°u tiÃªn cao', 'ngÆ°á»i giÃ ', 'tráº» em', 'bÃ  báº§u']): priority = 'HIGH'
        
        # 2. Extract Phones
        phones = []
        matches = re.findall(phone_pattern, line)
        for p in matches:
            clean = re.sub(r'[^\d]', '', p)
            if 9 <= len(clean) <= 11:
                if len(clean) == 10: phones.append(f"{clean[:4]} {clean[4:7]} {clean[7:]}")
                else: phones.append(clean)
                
        # 3. STRICT LOCATION EXTRACTION
        # Láº¥y content gá»‘c, bá» sá»‘ Ä‘iá»‡n thoáº¡i
        content_for_extract = line
        for p in matches:
            content_for_extract = content_for_extract.replace(p, '')
            
        # Ãp dá»¥ng hÃ m trÃ­ch xuáº¥t
        strict_address = extract_location_strict(content_for_extract)
        
        if len(strict_address) < 3: continue # QuÃ¡ ngáº¯n -> Bá»
        
        # 4. Determine Area
        area = 'KhÃ¡c'
        for k in sorted_areas:
            if k.lower() in strict_address.lower(): # Check trÃªn Ä‘á»‹a chá»‰ Ä‘Ã£ clean
                area = area_keywords[k]
                break
        
        # Fallback area check
        if area == 'KhÃ¡c':
            match = re.search(r'(xÃ£|thÃ´n|phÆ°á»ng)\s+([A-ZÄ][a-zÃ -á»¹]+)', strict_address)
            if match:
                # Logic map thÃªm náº¿u cáº§n
                pass

        raw_cases.append({
            "id": current_id,
            "content": strict_address, # LÆ¯U Äá»ŠA CHá»ˆ ÄÃƒ CLEAN
            "original_content": content_for_extract.strip(), # LÆ°u láº¡i gá»‘c Ä‘á»ƒ tham kháº£o náº¿u cáº§n
            "phones": phones,
            "area": area,
            "priority": priority,
            "isRescued": False
        })
        current_id += 1
        
    # Deduplicate (Blocking by Area)
    print(f"ğŸ“ Parsed {len(raw_cases)} cases. Deduplicating...")
    cases_by_area = {}
    for c in raw_cases:
        if c['area'] not in cases_by_area: cases_by_area[c['area']] = []
        cases_by_area[c['area']].append(c)
        
    unique_cases = []
    priority_order = {'CRITICAL': 0, 'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}
    
    for area, group in cases_by_area.items():
        group.sort(key=lambda x: x['content']) # Sort by content
        skip = set()
        for i in range(len(group)):
            if i in skip: continue
            curr = group[i]
            dups = [curr]
            # Check neighbors
            for j in range(i+1, min(i+15, len(group))):
                if j in skip: continue
                if is_duplicate(curr, group[j]):
                    dups.append(group[j])
                    skip.add(j)
            
            # Merge: Keep highest priority, merge phones
            best = min(dups, key=lambda x: priority_order[x['priority']])
            all_phones = set()
            for d in dups: all_phones.update(d['phones'])
            best['phones'] = sorted(list(all_phones))[:5]
            unique_cases.append(best)
            
    # Re-index
    for i, c in enumerate(unique_cases, 1): c['id'] = i
    
    # Save
    with open('rescue-app/src/data.json', 'w', encoding='utf-8') as f:
        json.dump(unique_cases, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… DONE! Saved {len(unique_cases)} clean locations.")
    
    # Preview
    print("\nğŸ” PREVIEW (Input -> Output):")
    for c in unique_cases[:10]:
        print(f"ğŸ“ {c['content']}")

if __name__ == "__main__":
    parse_strict()
